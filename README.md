# multimodal-fusion
This repository contains codes of our some recent works aiming at multimodal fusion, including Divide, Conquer and Combine: Hierarchical Feature Fusion Network with Local and Global Perspectives for Multimodal Affective Computing, Locally Confined Modality Fusion Network With a Global Perspective for Multimodal Human Affective Computing, etc.

The data are originally released in https://github.com/A2Zadeh/CMU-MultimodalSDK and are finally provided in https://github.com/soujanyaporia/multimodal-sentiment-analysis. If you need to use these data, please cite their corresponding papers.

Belows are the detailed introductions of our fusion methods:
1. HFFN

If you need to use the codes, please cite our paper:
Mai S, Hu H, Xing S. Divide, Conquer and Combine: Hierarchical Feature Fusion Network with Local and Global Perspectives for Multimodal Affective Computing[C]//Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. 2019: 481-492.
